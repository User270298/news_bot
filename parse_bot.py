import requests
from bs4 import BeautifulSoup
import openai
import asyncio
from aiogram import Bot, Dispatcher
import os
from dotenv import load_dotenv
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import undetected_chromedriver as uc
import time
load_dotenv()

openai.api_key = os.getenv('openai.api_key')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
API_TOKEN = os.getenv('API_TOKEN')

CHANNEL_ID=os.getenv('CHANNEL_ID')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot = Bot(token=API_TOKEN)
dp = Dispatcher()


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
def oilworld_parser(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_div = soup.find('div', class_='row l-col-news')
    links = news_div.find_all('a')

    set_href = []
    for link in links:
        href = link.get('href')
        set_href.append(f'https://www.oilworld.ru{href}')
    count=0
    oilworld = []
    for i in set(set_href):
        response = requests.get(i)
        soup = BeautifulSoup(response.text, 'html.parser')
        news_div = soup.find('div', class_='pad-left-page')
        title = news_div.find('h2').text
        text_ = news_div.find('div', class_='l-font').text
        oilworld.append(f"Title:\n{title}\nText: {text_}")
        if count == 8:
            break
        count+=1
    return '\n'.join(oilworld[:8])  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 6 –Ω–æ–≤–æ—Å—Ç–∏


def zol_parser(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_div = soup.find('table', class_='news_block news_list')
    news_div = news_div.find_all('a', class_='news_block')

    set_href = []
    for news in news_div[:6]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–æ 6 –Ω–æ–≤–æ—Å—Ç–µ–π
        href = news.get('href')
        set_href.append(f'https://www.zol.ru{href}')
    count=0
    zol = []
    for i in set_href:
        response = requests.get(i)
        soup = BeautifulSoup(response.text, 'html.parser')
        news_div = soup.find('div', class_='content')
        title = news_div.find('h1', class_='news_one_h1').text
        text_ = news_div.find('div', class_='text_content news_one').text
        zol.append(f"Title:\n{title}\nText: {text_}")
        if count == 8:
            break
        count+=1
    return '\n'.join(zol[:8])  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –Ω–æ–≤–æ—Å—Ç–∏


def prozerno_parser(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_div = soup.find('ul', class_="latestnews dark")
    news_div = news_div.find_all('a')
    set_href = []
    for news in news_div[:6]:
        href = news.get('href')
        set_href.append(f'https://prozerno.ru{href}')
    prozerno = []
    count=0
    for i in set_href:
        response = requests.get(i)
        soup = BeautifulSoup(response.text, 'html.parser')
        news_div = soup.find('div', class_='item-page')
        title = news_div.find('h2').text
        text_ = news_div.find_all('p')
        prozerno.append(f"Title:\n{title}\n" + f'Text: {"".join([p.text for p in text_])}')
        if count == 8:
            break
        count += 1
    return ''.join(prozerno[:8])  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –Ω–æ–≤–æ—Å—Ç–∏


def investing_parser(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_div = soup.find('div', class_="col col--xs")
    # news_div = news_div.find_all('aside', class_="aside")
    news_div = news_div.find_all('a')

    set_href = []
    for news in news_div[:6]:
        href = news.get('href')
        set_href.append(f'https://www.agroinvestor.ru{href}')
        # print(href)
    investing = []
    count=0
    for i in set_href[:-1]:
        # print(i)
        response = requests.get(i)
        soup = BeautifulSoup(response.text, 'html.parser')
        news_div = soup.find('div', class_='article__header-content')
        # print(news_div)
        title = f'{news_div.find('h1').text}\n{news_div.find('h2').text}'
        # print(title)
        text_ = soup.find('div', class_='col col--xl')
        # print(text_)
        text_ = text_.find_all('p')
        # print(text_)
        if count == 8:
            break
        count+=1
        texts = ''
        for txt in text_:
            texts += txt.text
        #
        investing.append(f"Title:\n{title}\n" + f'Text: {"".join([p.text for p in text_])}')
    # print(investing)
    return ''.join(investing[:8])


def aemcx(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        news = []
        posts = soup.find_all('div', class_='w-post-elm post_image usg_post_image_1 stretched')
        for post in posts:
            href = post.find('a')['href']
            news.append(href)
        aemcx = []
        count=0
        for new in news:
            response = requests.get(new)
            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.find('h2', class_='w-post-elm post_title align_left entry-title color_link_inherit')
            title=title.text
            if count==8:
                break
            count += 1
            text_ = soup.find_all('div', class_='w-post-elm post_content')
            aemcx.append(f"Title:\n{title}\n" + f'Text: {"".join([p.text for p in text_])}')
        return ''.join(aemcx[:8])
    else:
        print(f"–û—à–∏–±–∫–∞: {response.status_code}")


def malayzya_tg(url):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.prettify())
        # pprint.pprint(soup.prettify())
        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
        # news=[]
        posts = soup.find_all('div', class_='tgme_widget_message text_not_supported_wrap js-widget_message')
        txt = ''
        for post in posts:
            txt += post.text
        return txt[1800:]

def kz_tg(url):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.prettify())
        # pprint.pprint(soup.prettify())
        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
        # news=[]
        posts = soup.find_all('div', class_='tgme_widget_message text_not_supported_wrap js-widget_message')
        txt = ''
        for post in posts:
            txt += post.text
        return txt[1800:]

def rusgrain_tg(url):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
        posts = soup.find_all('div', class_='tgme_widget_message text_not_supported_wrap js-widget_message')
        txt = ''

        for post in posts:
            # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ —Å –Ω–µ–Ω—É–∂–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            for unwanted in post.find_all(['div', 'a'],
                                          class_=['tgme_widget_message_forwarded_from', 'tgme_widget_message_footer']):
                unwanted.decompose()

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –≤ –∏—Ç–æ–≥–æ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            txt += post.text.strip()

        return txt[1800:]

def minselhoz_tg(url):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # print(soup.prettify())
        # pprint.pprint(soup.prettify())
        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
        # news=[]
        posts = soup.find_all('div', class_='tgme_widget_message text_not_supported_wrap js-widget_message')
        txt = ''
        for post in posts:
            txt += post.text
        return txt[1800:]

# –§—É–Ω–∫—Ü–∏—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–∞ __del__ –¥–ª—è undetected_chromedriver
def selenium_modified_del(self):

    try:
        self.service.process.kill()
    except Exception:  # noqa
        pass
    try:
        self.quit()
    except OSError:
        pass


uc.Chrome.__del__ = selenium_modified_del


def get_html_data(time_sleep):

    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument("--incognito")
    chrome_options.add_argument('--blink-settings=imagesEnabled=false')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞
    driver = uc.Chrome(options=chrome_options)

    website_html_content = []

    try:
        url = 'https://finviz.com/news.ashx'
        driver.get(url)

        # –Ø–≤–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–æ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, 'body'))
        )

        time.sleep(time_sleep)  # –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        html_content = driver.page_source
        website_html_content.append(html_content)

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö: {e}")

    finally:
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞
        driver.quit()

    return website_html_content


def parse_html(html_content):

    soup = BeautifulSoup(html_content, 'html.parser')

    # –ü—Ä–∏–º–µ—Ä –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö - –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_items = soup.find_all('a', class_='nn-tab-link')
    data = []

    for item in news_items:
        title = item.text.strip()
        link = item['href']
        data.append({'Title': title, 'Link': link})

    return data


def get_article_details(driver, link):

    driver.get(link)

    # –Ø–≤–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–æ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, 'body'))
    )

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
    title = soup.find('title').text if soup.find('title') else 'No Title'
    paragraphs = soup.find_all('p')
    content = ' '.join(p.text for p in paragraphs).strip()

    return {'Title': title, 'Content': content}


def collect_articles_text(time_sleep):

    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument("--incognito")
    chrome_options.add_argument('--blink-settings=imagesEnabled=false')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞
    driver = uc.Chrome(options=chrome_options)

    all_articles_text = ""
    articles_collected = 0  # –°—á—ë—Ç—á–∏–∫ —Å—Ç–∞—Ç–µ–π

    try:
        html_contents = get_html_data(time_sleep)
        for html in html_contents:
            parsed_data = parse_html(html)
            for entry in parsed_data:
                link = entry['Link']
                article_details = get_article_details(driver, link)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                if article_details['Content'].strip():
                    all_articles_text += f"Title: {article_details['Title']}\n"
                    all_articles_text += f"Content: {article_details['Content']}\n\n"

                    articles_collected += 1
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –ª–∏–º–∏—Ç–∞ –≤ 10 —Å—Ç–∞—Ç–µ–π
                    if articles_collected >= 10:
                        break

            # –ï—Å–ª–∏ 10 —Å—Ç–∞—Ç–µ–π —É–∂–µ —Å–æ–±—Ä–∞–Ω–æ, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–π —Ü–∏–∫–ª
            if articles_collected >= 10:
                break

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö: {e}")

    finally:
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞
        driver.quit()

    return all_articles_text

def agroinvestor(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        news = []
        posts = soup.find_all('div', class_='news__item news__item--small')
        for post in posts[:1]:
            href = post.find('a')['href']
            href=f'https://www.agroinvestor.ru{href}'

        response = requests.get(href)
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('div', class_='article__body')
        # print(title.text)
        return(title)

def minselhoz_tg(url):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ò—â–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã
        posts = soup.find_all('div', class_='tgme_widget_message text_not_supported_wrap js-widget_message')

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 –ø–æ—Å—Ç–æ–≤
        last_six_posts = posts[:6]

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã –ø–æ—Å—Ç–æ–≤
        txt = ''
        for post in last_six_posts:
            # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ —Å –Ω–µ–Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            for unwanted in post.find_all(['div', 'span', 'a'],
                                          class_=['tgme_widget_message_footer',  # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–µ–º
                                                  'tgme_widget_message_views',   # –£–¥–∞–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                                                  'tgme_widget_message_meta',    # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º
                                                  'tgme_widget_message_forwarded_from',  # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–∞–Ω–∞–ª–∞
                                                  'tgme_widget_message_service',
                                                  'tgme_widget_message_reply_header',  # –£–¥–∞–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è "Please open Telegram to view this post"
                                                  ]):
                unwanted.decompose()

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –≤ –∏—Ç–æ–≥–æ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            txt += post.text.strip() + '\n\n'

        return txt

def converted_file(url):
    with open(url, 'r', encoding='utf-8') as file:
        return file.read()


def news_archive(url, text):
    with open(url, 'w', encoding='utf-8') as file:
        file.write(text)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ OpenAI API
async def fetch_news():
    site_1 = oilworld_parser('https://www.oilworld.ru/news/')
    site_2 = prozerno_parser('https://prozerno.ru/')
    site_3 = zol_parser('https://www.zol.ru/news/grain/')
    site_4 = investing_parser('https://www.agroinvestor.ru')
    site_5 = aemcx("https://aemcx.ru/?ysclid=m0dfoaxnro723127004")
    site_6 = malayzya_tg("https://t.me/s/oilpalm")
    site_7 =collect_articles_text(3)
    site_8 =kz_tg('https://t.me/s/ElDalakz')
    site_9 =rusgrain_tg('https://t.me/s/rusgrain')
    site_10 =agroinvestor("https://www.agroinvestor.ru/archive/")
    site_11=minselhoz_tg('https://t.me/s/minselhozrf')
    site_12=minselhoz_tg('https://t.me/s/mcxae')
    prompt = (f"–ù–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–æ–≤:\n\nOilworld:\n{site_1}\n\nProzerno:\n{site_2}\n\nZol.ru:\n{site_3}\nagroinvestor.ru:\n{site_4}"
              f"\naemcx.ru:\n{site_5}\nmalayzya: \n{site_6}\nfinviz.ru\n{site_7}\nkz.ru:\n{site_8}\nrusgrain:\n{site_9}\nagroinvestor\n{site_10}\n"
              f"minselhozrf\n{site_11}\nminselhoz.ru:\n{site_12}")   #
    news_archive('news_archive.txt', prompt)
    file = converted_file(r'example.txt')
    try:
        response = await openai.ChatCompletion.acreate(
            model='gpt-4o-mini',
            messages=[
                {"role": "system", "contenf":f'''You are a news analyst. Summarize the key points from the news.
                                                For each news item, include the source of the information below the summary.
                                                The title is not needed, let's have the text in Russian
                                                Omit the date.
                                                Only share news about vegetable oils; skip anything related to butter or dairy.
                                                Place oil-related news last in the list.
                                                Avoid using asterisks (*) or numbering bullet points.
                                                Use emojis in the text to make it more engaging.
                                                Here are examples of how the news should be presented: {file}.'''},
                {"role": "user", "content": prompt},
            ],
            max_tokens=3000
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ OpenAI API: {e}")
        return None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–∞–Ω–∞–ª Telegram
async def send_news_to_channel(news):
    if news:
        await bot.send_message(CHANNEL_ID, f'''AGROCOR INFORMS üåæ\n\n{news}\n\nü§ùOFFERS ‚û°‚û°‚û° https://t.me/+F8OsiZiNLg00NTY6\nüö¢ VESSEL CATCHER ‚û°‚û°‚û° https://t.me/+AqsaM2xqhS44NDQy\nüî•üí∞ TRADER‚ÄôS STORIES ‚û°‚û°‚û° https://t.me/+bzwCOPgtSWMyNTQ
''', disable_web_page_preview=True)
    else:
        await bot.send_message(CHANNEL_ID, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç.")


# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
async def scheduled_news_sender(interval):
    while True:
        news = await fetch_news()
        await send_news_to_channel(news)
        await asyncio.sleep(interval)  # –ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)


# –û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è –±–æ—Ç–∞ –∏ –∑–∞–¥–∞—á–∏
async def main():
    asyncio.create_task(scheduled_news_sender(3600 * 24))  # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∫–∞–∂–¥—ã–µ 60 –º–∏–Ω—É—Ç
    await dp.start_polling(bot)  # –£–∫–∞–∑—ã–≤–∞–µ–º –±–æ—Ç–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ polling


# –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã
if __name__ == '__main__':
    asyncio.run(main())
